1. 기계 학습에서 학습이란 무엇인지를 정리하시오(2점). 
( 가중치, 손실함수가 무엇인지를 정리하고, 데이터, 가중치, 손실함수를 이용하여 학습이 무엇인지를 정리함.) 

모델은 입력 데이터와 가중치를 사용하여 예측을 수행한다. 
가중치(Weight)는 모델이 데이터의 특성을 고려하여 예측을 조정하는 매개변수이다. 
즉, 모델은 학습 과정에서 가중치를 조정하고 최적의 값으로 수렴하도록 노력한다.

손실 함수는 모델의 예측과 실제 정답 간의 차이를 측정하는 함수이다. 
이 함수는 모델이 얼마나 정확한 예측을 하는지를 평가하는 데 사용된다. 
학습 과정에서 모델은 손실 함수를 최소화하기 위해 가중치를 조정한다.

따라서 학습은 주어진 데이터를 사용하여 모델의 가중치를 최적화하고, 손실 함수를 최소화하도록 하는 과정이다.
학습이 성공적으로 수행되면 모델은 주어진 입력에 대해 더 나은 예측을 수행할 수 있게 된다.

2. 확률적 경사 하강법의 소스 코드를 분석하시오(2점). (Page 173, 4장 모델 훈련, 첨부 파일 참조)
n_epochs = 50
t0, t1 = 5, 50 # 학습 스케줄 하이퍼파라미터

def learning_schedule(t):
	return t0 / (t + t1)

theta = np.random.randn(2,1) # 무작위 초기화

for epoch in range(n_epochs):
	for i in range(m):
		random_index = np.random.randint(m)
		xi = X_b[random_index:random_index+1]
		yi = y[random_index:random_index+1]
		gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
		eta = learning_schedule(epoch * m + i)
		theta = theta - eta * gradients

확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용하여 선형 회귀(Linear Regression) 모델을 학습하는 예제이다. 
확률적 경사 하강법은 대규모 데이터셋을 다룰 때 효율적인 학습 방법으로 사용된다.

- n_epochs
전체 학습 과정(epoch)의 수를 나타내는 변수로, 50으로 설정되어 있다. 
이는 전체 데이터셋을 50번 반복하여 학습한다는 의미이다.

- t0와 t1 
학습 스케줄(learning schedule) 하이퍼파라미터이다. 
학습률(learning rate)을 동적으로 조정하는 데 사용된다.

- learning_schedule(t) 
학습 스케줄 함수로, 현재 학습 단계 t를 인자로 받아 학습률(eta)를 계산한다. 
이 함수는 t0 / (t + t1)의 형태로 학습률을 계산한다.

- theta 
초기에 무작위로 설정한 선형 회귀 모델의 가중치 벡터이다.

- for epoch in range(n_epochs): 
전체 학습 과정(에포크)을 반복한다.

- for i in range(m): 
데이터셋의 모든 데이터 포인트를 순환한다.

- random_index = np.random.randint(m) 
무작위로 데이터 포인트를 선택하기 위해 m 개의 데이터 포인트 중에서 무작위 인덱스를 선택한다. 
이것이 SGD의 핵심 아이디어 중 하나이다.

- xi와 yi 
선택된 데이터 포인트의 입력 특성과 정답을 나타낸다.

- gradients 
현재 선택된 데이터 포인트를 사용하여 손실 함수를 최소화하기 위한 기울기(그래디언트)를 계산한다. 
선형 회귀의 경우 2 * xi^T * (xi * theta - yi)로 계산된다.

- eta 
학습률(learning rate)을 learning_schedule 함수를 통해 동적으로 계산한다.

- theta = theta - eta * gradients 
현재 가중치를 학습률과 기울기를 사용하여 업데이트한다. 
이것이 확률적 경사 하강법의 핵심이다.
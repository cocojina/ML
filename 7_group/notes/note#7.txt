1. 차원 축소란?
차원 축소는 기계학습에서 데이터의 특성을 유지하면서 데이터의 차원을 줄이는 프로세스를 말한다.
이는 데이터의 복잡성을 감소시키고 계산 효율성을 높이는 데 도움이 된다. 
주로 고차원 데이터에서 저차원으로의 매핑을 통해 이루어진다. 

차원 축소의 목표는 계산 효율성 향상, 노이즈 제거, 시각화, 과적합 감소이다.

- 계산 효율성 향상 
고차원 데이터는 계산상의 비용이 높을 수 있다. 
차원을 축소하면 계산 비용이 감소하면서 모델의 학습 및 예측 속도가 향상될 수 있다.
- 노이즈 제거 
데이터의 고차원 특성 중에는 유용한 정보가 아닌 노이즈가 섞여 있을 수 있다. 
차원 축소를 통해 노이즈를 제거하고 중요한 패턴에 집중할 수 있다.
- 시각화 
차원 축소는 데이터를 시각적으로 이해하고 해석하는 데 도움이 된다. 
저차원으로 표현된 데이터는 쉽게 시각화할 수 있으며, 이를 통해 데이터의 구조와 패턴을 파악할 수 있다.
- 과적합 감소
차원이 많으면 모델이 훈련 데이터에 과적합되기 쉬워진다. 
차원 축소는 이러한 과적합 문제를 완화할 수 있다.

차원 축소에는 주로 피처 선택, 피처 추출의 두 가지 접근 방식이 있다.
- 피처 선택 (Feature Selection) 
기존의 특성 중에서 가장 중요한 특성들을 선택하여 사용하고, 나머지는 버리는 방식이다. 
주로 특성의 중요도를 평가하여 선택한다.
- 피처 추출 (Feature Extraction)
기존의 특성들을 조합하거나 변환하여 새로운 특성을 만드는 방식이다. 
주로 주성분 분석 (PCA)나 t-SNE와 같은 기법을 사용한다.

가장 널리 사용되는 차원 축소 기법 중 하나는 PCA(Principal Component Analysis)이다. 
PCA는 데이터의 분산을 최대로 하는 새로운 특성 축을 찾아 차원을 축소하는 방법이다. 
PCA를 사용하면 데이터의 주요한 구조를 보존하면서 차원을 효과적으로 축소할 수 있다.

2. 차원 축소를 위한 접근 방법
차원을 감소시키는 두 가지 주요한 접근법은 투영 (Projection)과 매니폴드 학습이다.

- 투영 (Projection)
투영은 고차원 데이터를 하위 차원 공간으로 옮겨 놓는 것을 의미한다. 
예를 들어, 3D 공간의 데이터를 2D 평면으로 투영하면 차원이 감소된다.
예시로는, 주성분 분석 (PCA)는 투영을 사용하는 차원 축소 기법 중 하나이다. 
PCA는 데이터의 분산을 최대화하는 새로운 축을 찾아 투영을 수행한다.
장점: 단순하며 계산 비용이 낮을 수 있다.
단점: 선형적인 투영만 가능하며, 비선형 구조를 잡아내기 어려울 수 있다.

- 매니폴드 학습 (Manifold Learning)
고차원 데이터가 실제로는 저차원 매니폴드(manifold)에 분포되어 있다고 가정하고, 이를 학습하여 차원을 축소하는 방법이다. 
매니폴드는 데이터가 놓여 있는 공간의 구조를 나타낸다.
예시로는, t-SNE (t-Distributed Stochastic Neighbor Embedding)는 매니폴드 학습의 한 예로, 데이터 간의 국소적인 구조를 보존하면서 차원을 축소한다.
장점: 비선형적인 구조를 잘 잡아낼 수 있다. 데이터의 복잡한 구조를 유지하면서 시각화에 많이 활용된다.
단점: 계산 비용이 높을 수 있고, 특히 고차원 데이터에 적용할 때 주의가 필요하다.

두 접근법은 각각의 특성과 상황에 따라 선택되며, 어떤 데이터 특성을 보존하고자 하는지에 따라 적절한 방법을 선택하는 것이 중요하다. 
투영은 주로 선형 구조를 다루는 데 적합하며, 매니폴드 학습은 비선형적인 데이터 구조를 잡아내는 데 유용하다.

3. PCA
주성분 분석(Principal Component Analysis, PCA)은 데이터의 차원을 축소하고 주요한 정보를 보존하는 선형 변환 기법 중 하나이다. 
주로 데이터의 분산을 최대화하는 방향으로 새로운 축을 찾아 데이터를 변환한다. 
PCA는 차원 축소 뿐만 아니라 데이터의 시각화, 노이즈 제거, 특성 추출 등 다양한 분야에서 활용된다.

PCA의 주요 단계는 다음과 같다.
- 데이터 중심화 (Centering) 
각 특성에서 평균을 빼서 데이터를 중심화한다. 
이는 새로운 좌표계의 원점을 데이터의 중심으로 옮기는 효과를 가진다.

- 공분산 행렬 계산 (Compute Covariance Matrix) 
중심화된 데이터를 사용하여 공분산 행렬을 계산한다. 
공분산은 두 변수 간의 상관 관계를 나타내며, PCA에서는 데이터의 분산과 관련이 있다.
- 고유값 분해 (Eigenvalue Decomposition)
공분산 행렬을 고유값과 고유벡터로 분해한다. 
고유값은 해당 고유벡터의 중요도를 나타내며, 높은 고유값을 갖는 고유벡터일수록 데이터의 분산을 많이 설명할 수 있다.
- 주성분 선택 (Select Principal Components) 
고유값이 큰 순서대로 고유벡터를 선택하여 주성분으로 사용한다. 
주성분은 원본 데이터의 분산을 가장 많이 보존하는 방향을 나타낸다.
- 새로운 좌표로 변환 (Transform to New Coordinates)
선택한 주성분으로 이루어진 행렬을 사용하여 원본 데이터를 새로운 좌표계로 변환한다. 
이 과정에서 차원이 축소된다.

PCA를 통해 얻은 주성분은 원본 데이터의 주요한 변동성을 잡아내므로, 적은 수의 주성분으로도 데이터의 중요한 특성을 잘 표현할 수 있다.
주성분은 서로 직교하므로, 주성분들은 데이터의 상관성을 최대한 적게 가지게 된다.

4. 커널 PCA
커널 주성분 분석(Kernel Principal Component Analysis, Kernel PCA)는 주성분 분석(PCA)을 비선형 데이터에 적용하기 위한 확장된 방법이다. 
기본적인 PCA는 선형 데이터에 적합하며, 데이터가 비선형 구조를 갖는 경우에는 유용하지 않을 수 있다. 
이때 커널 PCA가 도움이 된다.

커널 PCA는 주로 비선형 특성을 갖는 데이터를 고차원 공간으로 매핑하여 선형 PCA를 적용하는 방식으로 동작한다. 
이를 가능하게 하는 핵심 아이디어는 커널 트릭(kernel trick)을 사용하는 것이다. 
커널 트릭은 비선형 매핑을 명시적으로 계산하지 않고, 두 데이터 포인트 간의 유사도(커널 함수)를 계산함으로써 고차원 특징 공간에서의 내적을 효과적으로 계산하는 기법이다.

커널 PCA의 주요 단계는 다음과 같다.

- 커널 행렬 계산 (Compute Kernel Matrix) 
주어진 데이터셋에 대한 커널 행렬을 계산한다. 
커널 행렬은 각 데이터 포인트 간의 유사도를 담고 있으며, 주로 가우시안 커널 또는 다항식 커널 등이 사용된다.
- 중앙화 (Centering)
커널 행렬을 중앙화한다.
- 고유값 분해 (Eigenvalue Decomposition)
중앙화된 커널 행렬을 사용하여 고유값과 고유벡터를 계산한다.
- 주성분 선택 (Select Principal Components)
고유값이 큰 순서대로 고유벡터를 선택하여 주성분으로 사용한다.
- 새로운 좌표로 변환 (Transform to New Coordinates)
선택한 주성분으로 이루어진 행렬을 사용하여 원본 데이터를 새로운 좌표계로 변환한다.

커널 PCA를 사용하면 원본 데이터가 선형적으로 분리되지 않는 경우에도 주성분을 추출하여 비선형 특징을 잘 표현할 수 있다. 
그러나 주의할 점은 커널 PCA는 계산 비용이 높고, 커널의 선택에 따라 결과가 크게 달라질 수 있다는 점이다. 
적절한 커널 함수를 선택하는 것이 중요하다.

5. LLE 
LLE(Locally Linear Embedding)는 비선형 차원 축소 알고리즘 중 하나로, 데이터의 지역적인 선형 구조를 보존하면서 저차원으로 투영하는 방법을 제공한다. 
LLE는 주로 비선형 매핑에 사용되며, 데이터가 매니폴드(manifold)로 표현될 때 효과적이다.

LLE의 주요 아이디어는 각 데이터 포인트를 이웃 데이터 포인트와 비교하여 주어진 데이터 포인트를 이웃들에 어떻게 선형적으로 잘 표현할 수 있는지를 찾는 것이다. 

LLE의 주요 단계
- 이웃 선택 (Neighbor Selection) 
각 데이터 포인트에 대해 가까운 이웃을 선택한다. 
이웃은 주로 거리 기반으로 선택되며, 가장 가까운 k개의 이웃을 선택하는 방식이 일반적이다.
- 이웃 간 선형 관계 학습 (Learn Linear Relationships among Neighbors)
각 데이터 포인트를 그 이웃들에 어떻게 가장 선형적으로 잘 표현할 수 있는지를 학습한다. 
이는 이웃 간의 가중치를 조절하여 선형 관계를 최대화하는 방식으로 이루어진다.
- 저차원 표현 생성 (Create Low-Dimensional Representation)
각 데이터 포인트를 이웃들에 대한 선형 관계를 이용하여 저차원으로 투영한다. 
이 때, 각 데이터 포인트의 저차원 표현은 이웃 간의 선형 관계를 최대한 보존하도록 한다.

LLE의 특징
- 지역적인 구조 보존 
LLE는 각 데이터 포인트를 이웃과의 지역적인 선형 구조를 유지하면서 차원을 축소한다. 
이는 매니폴드가 복잡한 비선형 구조를 갖는 데이터에서 특히 유용하다.
- 이웃의 개수와 거리에 민감 
LLE의 성능은 이웃의 개수(k)와 거리에 영향을 받는다. 
적절한 k값을 선택하는 것이 중요하며, 이는 주어진 데이터의 특성에 따라 다를 수 있다.
- 계산 비용이 높음 
LLE의 계산 비용은 주로 이웃 선택과 이웃 간의 선형 관계 학습 단계에서 발생하므로, 큰 데이터셋에 대해 계산 비용이 높을 수 있다.

LLE는 비선형 차원 축소의 유용한 도구 중 하나이며, 
특히 고차원 데이터셋이 복잡한 매니폴드 구조를 갖는 경우에 효과적이다.
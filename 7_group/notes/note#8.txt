1. 비지도 학습이란?
비지도 학습(Unsupervised Learning)은 기계 학습의 한 유형으로, 레이블이 지정되지 않은 데이터를 사용하여 모델을 훈련하는 방법이다.
다시 말해, 비지도 학습은 데이터에 대한 사전 정보 없이 데이터의 구조나 패턴을 찾으려고 시도한다. 
이는 주로 데이터 간의 관계, 숨겨진 구조, 군집 등을 발견하는 데 사용된다.

주요한 비지도 학습 알고리즘에는 다음과 같은 것들이 있다.
- 군집화(Clustering) 
데이터를 유사한 그룹 또는 군집으로 그룹화하는 작업이다. 
k-평균 군집화나 계층적 군집화 등이 있다.
- 차원 축소(Dimensionality Reduction) 
데이터의 특징을 줄이면서 중요한 정보를 보존하는 작업이다. 
대표적으로 주성분 분석(PCA)이나 t-SNE가 있더.
- 자기 지도 학습(Self-Supervised Learning) 
레이블이 없는 데이터에서 스스로 레이블을 생성하여 학습하는 방식이다. 
예를 들면, 오토인코더가 있다.
- 토픽 모델링(Topic Modeling)
문서 집합에서 주제를 추출하는 기법으로, Latent Dirichlet Allocation (LDA)가 널리 사용된다.
- 신경망 생성 모델(Generative Neural Networks) 
새로운 데이터를 생성하는 모델로, 생성적 적대 신경망(GAN)이 대표적이다.

비지도 학습은 데이터의 특징이나 패턴을 발견하고, 이를 통해 레이블이 없는 데이터에 대한 의미 있는 정보를 추출하는 데 사용된다. 
이는 주로 데이터가 많이 있지만 레이블이 부족한 경우나 레이블이 비용이 많이 드는 경우에 유용하다.

2. 군집이란?
군집(Clustering)은 비지도 학습의 한 유형으로, 유사한 속성이나 패턴을 가진 데이터를 그룹화하는 과정이다. 
군집화는 데이터 내의 숨겨진 구조나 패턴을 찾아내어 그룹 간의 유사성을 도출한다. 
이러한 그룹은 클러스터(cluster)라고 불리며, 클러스터 내의 데이터 포인트는 서로 비슷하거나 관련성이 높은 특성을 공유한다.

군집화의 주요 목적은 다음과 같다.
- 유사한 데이터 그룹화
비슷한 특성이나 패턴을 가진 데이터를 동일한 클러스터로 그룹화하여 데이터의 내부 구조를 이해한다.
- 패턴 발견 
데이터의 숨겨진 패턴이나 구조를 찾아내어 해당 정보를 활용할 수 있도록 돕는다.
- 이상치 탐지 
비정상적인 데이터 포인트를 식별하여 이상치(Outlier)를 감지하는 데에도 사용될 수 있다.

군집화 알고리즘의 몇 가지 예시는 다음과 같다.
- k-평균 군집화 (k-Means Clustering)
주어진 데이터를 k개의 클러스터로 나누는 방식으로, 각 클러스터의 중심을 계산하고 해당 중심에 가장 가까운 데이터 포인트를 클러스터에 할당한다.
- 계층적 군집화 (Hierarchical Clustering) 
계층적으로 클러스터를 형성하며, 각 단계에서 가까운 클러스터를 병합하거나 나누어 전체 계층 구조를 형성한다.
- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
데이터의 밀도를 기반으로 클러스터를 형성하며, 밀도가 높은 지역에 클러스터를 형성하고 비교적 낮은 지역은 노이즈로 처리한다.
- Mean Shift Clustering 
데이터 포인트가 밀도가 가장 높은 방향으로 이동하면서 클러스터를 형성하는 방식이다.

군집화는 다양한 분야에서 활용되며, 예를 들어 고객 세분화, 이미지 분할, 자연어 처리에서의 주제 모델링 등 다양한 응용 분야에서 사용된다.

3. k평균 군집화란?
k-평균 군집화(k-Means Clustering)는 가장 일반적으로 사용되는 군집 알고리즘 중 하나로, 주어진 데이터를 k개의 클러스터로 그룹화하는 비지도 학습 알고리즘이다. 
각 클러스터는 중심(centroid)이라 불리는 가상의 지점을 가지며, 각 데이터 포인트는 가장 가까운 중심에 할당된다.

k-평균 군집화의 주요 단계는 다음과 같다.
- 클러스터 개수(k) 선택 
먼저 사용자는 몇 개의 클러스터로 데이터를 그룹화할 것인지 결정해야 한다. 
이것이 k이다.
- 중심 초기화 
각 클러스터의 중심을 무작위로 선택하거나, 데이터 포인트 중에서 k개를 선택하여 초기 중심을 설정한다.
- 할당(Assignment) 
각 데이터 포인트를 가장 가까운 중심에 할당한다. 
일반적으로는 유클리드 거리를 사용하여 가장 가까운 중심을 찾는다.
- 중심 업데이트 
각 클러스터의 중심을 해당 클러스터에 속한 모든 데이터 포인트의 평균 위치(중심)으로 업데이트한다.
- 할당과 중심 업데이트 반복 
할당과 중심 업데이트 단계를 반복하면서 클러스터의 중심이 수렴할 때까지 진행한다.

알고리즘이 수렴하면, 각 데이터 포인트는 하나의 클러스터에 속하게 되며, 각 클러스터는 중심을 가지게 된다. 
k-평균 군집화는 계산 비용이 낮고 구현이 간단하며, 많은 경우에 효과적으로 사용된다.

그러나 몇 가지 주의할 점이 있다.
- 초기 중심의 선택에 따라 결과가 달라질 수 있다.
- 클러스터의 개수 k를 사전에 정해주어야 한다.
- 이상치에 민감하며, 클러스터의 모양이 원형에 가깝게 가정한다.
- k-평균 군집화는 데이터 마이닝, 이미지 분할, 고객 세분화 등 다양한 분야에서 활용되고 있다.

4. 군집을 사용한 이미지 분할
이미지 분할은 이미지를 여러 부분 또는 객체로 나누는 작업을 의미한다. 
군집화를 사용한 이미지 분할은 비슷한 픽셀 값 또는 특징을 갖는 영역을 그룹화하여 이미지를 세분화하는 방법 중 하나이다. 
여기서는 k-평균 군집화를 활용한 이미지 분할에 대해 설명한다.

- 데이터 준비 
이미지를 픽셀 단위로 나누고, 각 픽셀의 RGB(또는 다른 색상 공간) 값을 사용하여 데이터를 구성한다. 
이때, 이미지의 각 픽셀이 데이터 포인트로 간주된다.
- k-평균 군집화 
k-평균 군집화 알고리즘을 적용하여 이미지 내의 픽셀을 k개의 클러스터로 그룹화한다. 
각 클러스터는 비슷한 색상이나 특징을 갖는 픽셀의 그룹이 된다. 
클러스터링은 주로 픽셀 간의 색상 유사성을 기반으로 이루어진다.
- 클러스터 레이블 할당 
각 픽셀은 가장 가까운 클러스터의 중심에 할당된다.
이때, 클러스터링된 결과를 기반으로 이미지를 세분화하면, 비슷한 색상 또는 특징을 가진 영역이 클러스터로 그룹화된다.
- 결과 확인 및 조정 
군집화 결과를 시각적으로 확인하고, 필요에 따라 클러스터의 수나 초기 중심을 조정하여 최종 이미지 분할 결과를 얻는다.

이미지 분할을 통해 객체의 경계를 식별하거나 이미지의 특정 부분에 대한 분석을 수행할 수 있다. 
또한, 이미지 분할은 컴퓨터 비전, 의료 영상 처리, 자율 주행 자동차 및 보안 시스템 등 다양한 응용 분야에서 활용된다.

5. 군집을 사용한 전처리
군집화는 전처리 단계에서 데이터를 그룹화하고 군집 간의 패턴을 찾아내는 데 사용될 수 있다. 
여러 가지 전처리 목적에 따라 군집화를 활용하는 방법이 다양하게 있다. 
아래는 몇 가지 군집화를 사용한 전처리 예시이다.

- 이상치 탐지
군집화를 사용하여 데이터의 정상 패턴을 학습하고, 이에 벗어나는 데이터를 이상치로 간주할 수 있다.
클러스터에 속하지 않는 데이터 포인트는 이상치로 처리할 수 있다.
- 데이터 축소
군집화를 통해 대표성 있는 대표점을 선정하거나 군집의 중심을 사용하여 데이터를 대표화할 수 있다.
데이터가 매우 많은 경우, 각 군집의 중심으로 데이터를 표현함으로써 원본 데이터를 더 적은 수의 대표점으로 표현할 수 있다.
- 클래스 레이블 생성
군집화 결과를 사용하여 새로운 클래스 레이블을 생성할 수 있다.
예를 들어, 군집화를 통해 텍스트 데이터의 주제를 찾아내고 각 군집에 대한 레이블을 생성할 수 있다.
- 특징 추출
군집화를 통해 데이터의 유사성을 고려하여 새로운 특징을 추출할 수 있다.
각 군집의 특징이 군집화된 데이터에 대한 새로운 특징으로 사용될 수 있다.
- 데이터 정제
군집화를 사용하여 데이터의 군집에 속하지 않는 이상치를 식별하고 제거하여 데이터를 정제할 수 있다.
노이즈를 제거하여 모델의 성능을 향상시킬 수 있다.
이러한 전처리 기법은 데이터의 특성과 목적에 따라 다르게 선택될 수 있다. 
군집화를 적용하기 전에 목적과 데이터의 특성을 고려하여 적절한 전처리 방법을 선택하는 것이 중요하다.

6. 군집을 사용한 준지도 학습
준지도 학습(Semi-Supervised Learning)은 데이터 중 일부만 레이블이 주어진 상태에서 학습하는 방법을 의미한다. 
군집을 사용한 준지도 학습은 이러한 레이블이 없는 데이터를 군집화하여 레이블을 예측하고, 이를 통해 모델을 향상시키는 방식으로 구현될 수 있다. 
아래는 군집을 사용한 준지도 학습의 기본 아이디어에 대한 설명이다.

- 데이터 군집화
레이블이 있는 일부 데이터와 레이블이 없는 다수의 데이터를 포함하는 전체 데이터 세트를 고려한다.
레이블이 없는 데이터에 대해 군집화 알고리즘을 적용하여 데이터를 서로 다른 군집으로 그룹화한다.
- 레이블 전파
레이블이 있는 데이터의 레이블을 해당 군집 내의 레이블이 없는 데이터에 전파한다.
군집 내의 각 데이터 포인트에 대해, 해당 군집의 레이블이 있는 데이터의 레이블을 사용하여 레이블을 예측하고 할당한다.
- 모델 학습
레이블이 있는 데이터와 전파된 레이블을 사용하여 지도 학습 모델을 학습한다.
모델은 이제 레이블이 있는 데이터와 예측된 레이블을 모두 사용하여 성능을 향상시킬 수 있다.
군집을 사용한 준지도 학습은 레이블이 없는 데이터를 레이블이 있는 데이터와 관련된 그룹으로 묶어 전파함으로써 모델의 학습에 활용한다. 
이 방식은 레이블이 부족한 상황에서 효과적일 수 있으며, 군집화를 통해 데이터 간의 유사성을 고려한 레이블 전파가 모델의 성능을 향상시킬 수 있다.

준지도 학습은 현실 세계에서 레이블을 얻기 어려운 상황에서 유용하며, 예를 들어 의료 영상, 언어 모델링 등 다양한 분야에서 적용될 수 있다.

7. DBSCAN이란?
DBSCAN(Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반 군집화 알고리즘으로, 데이터의 밀도를 고려하여 클러스터를 형성힌디. 
이 알고리즘은 데이터 포인트의 밀도가 높은 지역을 클러스터로 간주하며, 이를 통해 노이즈 데이터를 식별할 수 있다.

DBSCAN의 주요 특징은 다음과 같다.
- 코어 포인트(Core Point) 
주어진 반경(epsilon, ε) 내에 최소 개수(minPts) 이상의 이웃이 있으면 해당 데이터 포인트를 코어 포인트로 간주한다.
- 이웃 포인트(Neighbor Point) 
코어 포인트에서 반경(ε) 내에 위치한 다른 데이터 포인트들을 이웃 포인트로 간주한다.
- 경계 포인트(Border Point) 
코어 포인트의 반경(ε) 내에 위치하면서 최소 이웃 개수(minPts) 이상의 이웃을 가지지 않는 데이터 포인트는 경계 포인트로 간주한다.
- 노이즈 포인트(Noise Point)
코어 포인트도 아니고 경계 포인트도 아닌 데이터 포인트를 노이즈 포인트로 간주한다.

DBSCAN 알고리즘의 동작 단계는 다음과 같다.
- 임의의 데이터 포인트 선택 
임의의 데이터 포인트를 선택하고, 해당 포인트가 코어 포인트인지 확인한다.

- 코어 포인트인 경우
해당 코어 포인트와 연결된 모든 이웃 포인트를 찾는다.
최소 이웃 개수(minPts) 이상의 이웃을 가지는 경우, 코어 포인트를 기준으로 클러스터를 형성하고 해당 클러스터에 속한 모든 포인트를 방문한 것으로 표시한다.
- 코어 포인트가 아닌 경우
다른 코어 포인트의 이웃이 될 때까지 다음 데이터 포인트를 선택하고, 이웃 포인트를 찾아 클러스터에 추가한다.

- 반복 
모든 데이터 포인트에 대해 위의 과정을 반복하면서 새로운 클러스터를 형성하거나 기존 클러스터에 포인트를 추가한다.

DBSCAN은 클러스터의 모양에 대한 가정이 없으며, 노이즈를 감지하고 처리할 수 있는 강력한 군집화 알고리즘 중 하나이다. 
그러나 초기 설정 값에 민감할 수 있고, 데이터의 밀도가 크게 변하는 경우에는 적절한 설정이 필요하다.

8. 가우시안 혼합 모델이란?
가우시안 혼합 모델(Gaussian Mixture Model, GMM)은 여러 개의 가우시안 분포를 혼합하여 데이터를 표현하는 확률 모델이다. 
이 모델은 주어진 데이터가 여러 개의 정규 분포에서 생성된 것으로 가정하며, 각 정규 분포를 가우시안 분포로 나타낸다.

GMM은 군집화와 밀접한 관련이 있다. 
각 가우시안 분포는 하나의 군집을 나타내며, 여러 개의 가우시안 분포를 혼합하여 전체 데이터를 설명한다. 
이러한 가우시안 분포의 혼합으로 인해 데이터가 여러 군집으로 나뉘어질 수 있다.

GMM의 주요 구성 요소와 작동 원리는 다음과 같다.
- 가우시안 분포
가우시안 분포는 정규 분포로도 알려져 있으며, 평균과 분산에 의해 특성화된다.
GMM은 여러 개의 가우시안 분포를 혼합하여 데이터를 설명한다.
- 혼합 계수(Mixing Coefficients)
각 가우시안 분포에 대한 혼합 계수는 해당 분포가 전체 혼합에서 차지하는 비중을 나타낸다.
혼합 계수의 합은 1이어야 한다
- 모수 추정
GMM은 평균, 분산, 혼합 계수와 같은 모수를 추정하는 과정을 거친다.
기대값-최대화(Expectation-Maximization, EM) 알고리즘을 통해 모수를 추정한다.
- 확률 예측
학습된 GMM은 주어진 데이터가 어떤 가우시안 분포에서 생성되었을 확률을 예측할 수 있다.
각 데이터 포인트가 각 가우시안 분포에 속할 확률을 계산한다.
GMM은 데이터에 대한 유연한 모델을 제공하며, 데이터가 여러 군집으로 나누어져 있을 때 유용하다. 
또한, 소프트 군집화(soft clustering)를 수행할 수 있어, 각 데이터 포인트가 여러 군집에 속할 수 있다. 
GMM은 주로 확률적인 방식으로 데이터를 모델링하고자 할 때 사용된다.

9. 가우시안 혼합을 사용한 이상치 탐지
가우시안 혼합 모델(Gaussian Mixture Model, GMM)은 이상치 탐지에 활용될 수 있는 강력한 도구 중 하나이다. 
이상치는 일반적으로 데이터의 정규 분포에서 벗어나는 패턴을 가진 데이터 포인트로 정의되며, GMM은 데이터를 여러 개의 가우시안 분포로 모델링하여 이러한 이상치를 탐지하는 데 사용된다.

아래는 GMM을 사용한 이상치 탐지의 기본적인 아이디어에 대한 설명이다.
- 모델 학습
GMM은 평균, 분산, 혼합 계수와 같은 모수를 추정하기 위해 EM(Expectation-Maximization) 알고리즘을 사용하여 학습된다.
정상적인 데이터셋으로 모델을 학습하면, GMM은 주어진 데이터셋을 가장 잘 설명하는 가우시안 분포들을 찾게 된다.
- 이상치 탐지
학습된 GMM을 사용하여 각 데이터 포인트가 주어진 분포에서 생성되었을 확률을 계산한다.
일반적으로, 이 확률이 낮은 데이터 포인트들이 이상치로 간주된다.
확률이 특정 임계값보다 작은 데이터 포인트는 이상치로 판단될 수 있다.
- 확률 분포 비교
일반적으로 정상 데이터와 이상치 데이터는 다른 분포를 가지므로, GMM은 이를 감지할 수 있다.
이상치는 모델이 학습한 정상적인 데이터와 크게 다른 확률 분포를 가질 가능성이 높다.
- 임계값 설정
이상치로 판단할 확률의 임계값은 사용자가 설정해야 한다.
임계값을 높게 설정하면 높은 신뢰성을 가진 이상치가 판별되지만, 낮게 설정하면 더 많은 이상치가 포함될 수 있다.
가우시안 혼합을 사용한 이상치 탐지는 데이터의 확률적인 특성을 고려하여 유연하게 이상치를 감지할 수 있다. 
그러나 이상치 데이터가 매우 드물거나 특이한 패턴을 가질 때는 일반적인 가우시안 혼합 모델이 적합하지 않을 수 있다. 
이런 경우에는 더 복잡한 이상치 탐지 기법이 필요할 수 있다.

10. 클러스터 개수 선택하기
이론적 정보 기준을 사용하여 군집 개수를 선택하는 방법은 모델의 복잡성과 데이터 적합도 사이의 균형을 찾는 방법 중 하나이다. 
이러한 기준 중 두 가지가 주로 사용되는데, 바로 Bayesian Information Criterion (BIC)과 Akaike Information Criterion (AIC)이다.

- BIC (Bayesian Information Criterion)
BIC는 모델의 복잡성에 대한 페널티를 부여하면서 로그 우도(likelihood)를 최대화하는 기준이다.
L은 모델의 파라미터 개수이고, m은 데이터의 샘플 수이다.
BIC는 학습할 파라미터가 많은 모델에게 벌칙을 가하므로, 더 간단하면서도 데이터를 잘 설명하는 모델을 선호한다.
-AIC (Akaike Information Criterion)
AIC도 로그 우도를 최대화하면서 모델의 복잡성에 대한 페널티를 부여하는 기준이다.
AIC도 BIC와 유사하게 모델의 복잡성에 대한 페널티를 가하며, 더 낮은 AIC 값이 더 좋은 모델을 의미한다.

클러스터 개수를 선택할 때는 군집 개수를 변경하면서 각 모델에 대한 AIC 또는 BIC 값을 계산하고, 가장 작은 AIC 또는 BIC 값을 가지는 군집 개수를 선택한다. 
이 값은 모델의 성능과 복잡성을 적절하게 균형있게 반영하게 된다.
그림에서 군집 수(K)에 따른 AIC 및 BIC의 값을 계산하여 보여주고 있으며, 이러한 그래프를 통해 AIC 또는 BIC가 최소가 되는 지점에서의 군집 개수를 선택하는 것이 바람직하다는 것을 확인할 수 있다. 
이 경우, 그림에서 K=3이 AIC 및 BIC가 최소인 지점으로 나타나 있다.

11. 베이즈 가우시안 혼합 모델
베이지안 가우시안 혼합 모델(Bayesian Gaussian Mixture Model, BGMM)은 가우시안 혼합 모델(Gaussian Mixture Model, GMM)의 확장된 형태로, 
베이지안 통계를 사용하여 데이터를 모델링하는 확률적인 방법이다. 
BGMM은 모수를 확률적으로 모델링하며, 불확실성을 측정할 수 있는 강력한 모델 중 하나이다.

여기에서는 BGMM의 주요 특징과 작동 방식에 대해 간략하게 설명하겠다.
- 확률적인 모델
BGMM은 모수(평균, 공분산 행렬 등)에 대한 불확실성을 확률 분포로 나타낸다.
모델의 각 가우시안 컴포넌트는 사후 분포를 가지며, 이를 통해 모델의 불확실성을 측정한다.
- 하이퍼파라미터
BGMM은 추가적인 하이퍼파라미터(예: 사전 분포의 모수)를 가진다.
하이퍼파라미터를 통해 데이터에 대한 선험적 지식을 통합하고, 더 안정적이고 일반적인 모델을 학습할 수 있다.
- 모델 선택
BGMM은 데이터에 대한 모델의 복잡성을 자동으로 조절할 수 있는 장점이 있다.
데이터에 따라 필요한 가우시안 컴포넌트의 수가 자동으로 조절되기 때문에, 사전에 군집 수를 정해주지 않아도 된다.
- Gibbs 샘플링 또는 Variational Inference 사용
BGMM은 베이지안 추론 방법 중 하나인 Gibbs 샘플링이나 Variational Inference를 사용하여 학습된다.
이러한 방법을 사용하면 데이터에 대한 사후 분포를 추정하고, 이를 통해 모델을 학습할 수 있다.
BGMM은 데이터의 복잡한 구조를 높은 유연성과 효과적으로 모델링할 수 있는 장점이 있다. 
또한, 클러스터 수를 사전에 정하지 않아도 되므로 데이터에 따라 동적으로 클러스터를 조절할 수 있는 특징이 있다.
